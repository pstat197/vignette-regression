---
title: "Ridge Regression"
format: html
editor: visual
---

## Ridge Regression

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Background

Ridge regression is a method used when we have highly correlated parameters. This issue of multicollinearity in linear regression can return biased results. Mulicollinearity will usually inflate the standard error of terms, resulting in inaccurate P values. Estimates will be biased and cannot be trusted. Ridge regression adds a penalization factor to linear regression. This penalization factor is the ridge parameter and is the tuning for ridge regression. Overall, ridge regression replace our residual estimation with residual plus a penalization term. So when we are minimizing our residuals the penalization term comes into effect.

```{r}
library(Matrix)
library(glmnet)
library(rsample)
library(dplyr)

```

The package we will be working with for ridge regression is glmnet. There are some requirements for data formatting for this package. This means that our response variable will have to be in a vector. Our explanatory variables will have to be in a matrix

```{r}
# Set the seed
set.seed(13)

partition <- life_clean %>%  # Splittin the data into testing and training 
  initial_split(prop = .8)

# Training Set
train_x <- training(partition) %>%
  select(-Life.expectancy) %>%
  as.matrix() # transforming our Xs into a matrix
train_y <- training(partition) %>%
  pull(Life.expectancy)

# Testing Set
test_x <- testing(partition) %>%
  select(-Life.expectancy) %>%
  as.matrix() # transforming our Xs into a matrix
test_y <- testing(partition) %>%
  pull(Life.expectancy)



```

Lambda is our tuning parameter for ridge regression meaning we are going to try to find an optimal lambda. Luckily glmnet has a cross validation feature, which will compare lambdas for us and find the one that minimizes the error term.

```{r}
lambdas = 10^seq(2, -3, by = -.1)
cv_ridge_reg = cv.glmnet(train_x,
                         train_y, 
                         alpha = 0, 
                         lambda = lambdas,
                         type.measure = "deviance")
optimal_lambda <- cv_ridge_reg$lambda.min
optimal_lambda
```

The next step is too look at our model performance now that we have selected an optimal lambda.

```{r}
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

# Performance metrics
data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
}

# Prediction and evaluation on train data
predictions_train <- predict(cv_ridge_reg, s = optimal_lambda, newx = train_x)
eval_results(train_y, predictions_train, training(partition))

# Prediction and evaluation on test data
predictions_test <- predict(cv_ridge_reg, s = optimal_lambda, newx = test_x)
eval_results(test_y, predictions_test, testing(partition))
```

The `echo: false` option disables the printing of code (only output is displayed).
