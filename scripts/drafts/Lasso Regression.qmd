---
title: "LASSO Regression"
format: html
editor: visual
---

## LASSO Regression

## Background

LASSO (Least Absolute Shrinkage and Selection Operator) regression is a method used when we have highly correlated parameters. This issue of multicollinearity in linear regression can return biased results. Mulicollinearity will usually inflate the standard error of terms, resulting in inaccurate P values. Estimates will be biased and cannot be trusted. Ridge regression adds a penalization factor to linear regression. This penalization factor is the ridge parameter and is the tuning for ridge regression. Overall, ridge regression replace our residual estimation with residual plus a penalization term. So when we are minimizing our residuals the penalization term comes into effect.

```{r}
library(dplyr)
library(glmnet)
library(caret)
library(tidyverse)
library(tidymodels)
```

The package we will be working with for LASSO regression is glmnet. There are some requirements for data formatting for this package. This means that our response variable will have to be in a vector and the predictor variables will have to be in matrix form

```{r}
# for reproducibility
set.seed(12)

# partition data
partitions <- life_clean %>%
  initial_split(prop = 0.8)

# Training Set
train_x <- training(partitions) %>%
  select(-Life.expectancy) %>%
  as.matrix()
train_y <- training(partitions) %>%
  pull(Life.expectancy)

# Testing Set
test_x <- testing(partitions) %>%
  select(-Life.expectancy) %>%
  as.matrix()
test_y <- testing(partitions) %>%
  pull(Life.expectancy)

```

Lambda is our tuning parameter for LASSO regression. We will find the optimal lambda value through cross validation.

```{r}
# lambda values we will choose from
lambdas <- 10^seq(2, -3, by = -.1)

# Perform cross validation to find best value of lambda
lasso_reg <- cv.glmnet(train_x, train_y, alpha = 1, lambda = lambdas)

# Best lambda value
best_lambda <- lasso_reg$lambda.min

best_lambda
```
